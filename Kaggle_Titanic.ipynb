{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/techisteps/KaggleCompetition/blob/main/Kaggle_Titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oeke8x-VM1q"
      },
      "source": [
        "# Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "-J3NxtURKSAv",
        "outputId": "d1f53a43-4ef8-43c2-baab-5ea5b1733d02"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-c1d8abbe-7567-4821-8a62-03612f38628c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-c1d8abbe-7567-4821-8a62-03612f38628c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HmZvt1JLFdG",
        "outputId": "08bf2bc7-c086-4785-cd01-d6525acf3b74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading titanic.zip to /content\n",
            "\r  0% 0.00/34.1k [00:00<?, ?B/s]\n",
            "\r100% 34.1k/34.1k [00:00<00:00, 36.4MB/s]\n",
            "Archive:  titanic.zip\n",
            "  inflating: gender_submission.csv   \n",
            "  inflating: test.csv                \n",
            "  inflating: train.csv               \n"
          ]
        }
      ],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "!kaggle competitions download -c titanic\n",
        "!unzip titanic.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQXJMIizVSPC"
      },
      "source": [
        "## Import libraries and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Xs_qThZ4LRUo"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mgyWBT0GLdAx"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')\n",
        "train_label = train_data[\"Survived\"]\n",
        "train_data.drop(\"Survived\", axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-AjRv6ioLvgL",
        "outputId": "02636113-f2fb-4b77-b413-dd29defa1e99"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "int64      5\n",
              "object     5\n",
              "float64    2\n",
              "Name: count, dtype: int64"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.dtypes\n",
        "train_data.dtypes.value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I3cpF9yVZ6q"
      },
      "source": [
        "# Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnX1iL-GQhDL",
        "outputId": "beb7a2a7-a0c5-4c4a-d285-a2ebb0b505b8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features_num is: Index(['PassengerId', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare'], dtype='object')\n",
            "features_obj is: Index(['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked'], dtype='object')\n"
          ]
        }
      ],
      "source": [
        "features_num = train_data.select_dtypes(include=[np.number]).columns\n",
        "features_obj = train_data.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "print(f\"features_num is: {features_num}\")\n",
        "print(f\"features_obj is: {features_obj}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1a4MYygObvPk",
        "outputId": "cd26c245-1f10-4611-81f2-a55bc7760e16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ticket\n",
              "              661\n",
              "PC             60\n",
              "CA             41\n",
              "A/             25\n",
              "STON/O         18\n",
              "SOTON/OQ       15\n",
              "W/C            10\n",
              "SC/PARIS        7\n",
              "FCC             5\n",
              "C               5\n",
              "SOC             5\n",
              "SC/Paris        4\n",
              "LINE            4\n",
              "SO/PP           3\n",
              "A               3\n",
              "PP              3\n",
              "SC/AH           2\n",
              "SOTON/O         2\n",
              "WE/P            2\n",
              "SW/PP           2\n",
              "P/PP            2\n",
              "FC              1\n",
              "SC/AHBasle      1\n",
              "A/S             1\n",
              "SP              1\n",
              "SC              1\n",
              "SCO/W           1\n",
              "Fa              1\n",
              "SOP             1\n",
              "WEP             1\n",
              "SC/A            1\n",
              "SO/C            1\n",
              "CA/SOTON        1\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ],
      "source": [
        "train_data[\"Ticket\"].str.extractall(r'(\\d+)')\n",
        "tmp = train_data[\"Ticket\"].str.extractall(r'(\\D+)').replace(r'^\\d+', '', regex=True).replace(r'/', '', regex=True).replace(r'\\.', '', regex=True)\n",
        "tmp = train_data[\"Ticket\"].str.replace(r'\\d+', '', regex=True).replace(r'\\.', '', regex=True).replace(r' ', '', regex=True)\n",
        "# tmp.columns = [\"Ticket\"]\n",
        "# tmp[\"Ticket\"].unique()\n",
        "tmp.value_counts()\n",
        "# type(tmp)\n",
        "\n",
        "# train_data[\"Ticket\"].unique()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3nINY32U-0s"
      },
      "source": [
        "## Feature Creation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6WH-6xMXVxl_"
      },
      "source": [
        "### Feature PassengerId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": true,
        "id": "Mwxoqm6sV-cI"
      },
      "outputs": [],
      "source": [
        "## PassengerId is a sequence number thus doest have much waitage for training. we also can not drop this column as test data will come with this column.\n",
        "## Best option will be to keep a constant value in this column 0 or 1.\n",
        "\n",
        "feature_PassengerId = train_data[\"PassengerId\"].copy()\n",
        "feature_PassengerId = feature_PassengerId.map(lambda x: 0)\n",
        "# feature_PassengerId\n",
        "# train_data\n",
        "# type(feature_PassengerId)\n",
        "\n",
        "# def featureMK_PassengerId(df: pd.DataFrame):\n",
        "#   feature_PassengerId = train_data[[\"PassengerId\"]].copy()\n",
        "#   feature_PassengerId = feature_PassengerId.map(lambda x: 0)\n",
        "#   return feature_PassengerId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "hNQy_U6haSFR",
        "outputId": "96191e93-faef-4ab5-ab61-798a4efd38a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.0.3'"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# tmp = featureMK_PassengerId(train_data)\n",
        "# type(tmp)\n",
        "pd.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhpuhazBVuVw"
      },
      "source": [
        "### Feature Age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "collapsed": true,
        "id": "suCTcPniTFHJ"
      },
      "outputs": [],
      "source": [
        "# train_data.isna().sum()\n",
        "  # train_data['Age'].isna().sum()\n",
        "# train_data.fillna(train_data[\"Age\"].mean(), inplace=True)\n",
        "\n",
        "def featureMK_Age(df: pd.DataFrame):\n",
        "  # feature_Age = df.fillna(df[\"Age\"].mean())[\"Age\"]\n",
        "  # return feature_Age\n",
        "\n",
        "  meanage = df[['Age', 'Sex', 'Pclass']].groupby(['Sex', 'Pclass']).mean()\n",
        "  # feature_Age = df[['Sex', 'Pclass']].merge(meanage, on=['Sex', 'Pclass'], how='left').fillna(df['Age'])['Age']\n",
        "  feature_Age = df[['Sex', 'Pclass']].merge(meanage, on=['Sex', 'Pclass'], how='left')\n",
        "  return feature_Age['Age']\n",
        "\n",
        "# featureMK_Age(train_data)\n",
        "# featureMK_Age(test_data)\n",
        "\n",
        "# train_data['Age'].isna().sum()\n",
        "# feature_Age = train_data.fillna(train_data[\"Age\"].mean())[\"Age\"]\n",
        "\n",
        "\n",
        "# train_data.columns\n",
        "# type(featureMK_Age(train_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhjbPwJsYjbn"
      },
      "source": [
        "### Feature Sex"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "vh0qHs7bYmmk"
      },
      "outputs": [],
      "source": [
        "# train_data[\"Sex\"].isna().sum()\n",
        "\n",
        "def featureMK_Sex(df: pd.DataFrame):\n",
        "  feature_Sex = df[[\"Sex\"]].copy()\n",
        "  feature_Sex[\"Male\"] = df[\"Sex\"].map(lambda x: 0 if x == \"male\" else 1)\n",
        "  feature_Sex[\"Female\"] = df[\"Sex\"].map(lambda x: 0 if x == \"female\" else 1)\n",
        "  # type(feature_Sex)\n",
        "  feature_Sex.drop(columns=[\"Sex\"], axis=1, inplace=True)\n",
        "  return feature_Sex\n",
        "\n",
        "# featureMK_Sex(train_data)\n",
        "# featureMK_Sex(test_data)\n",
        "\n",
        "# feature_Sex = train_data[[\"Sex\"]].copy()\n",
        "# feature_Sex[\"Male\"] = train_data[\"Sex\"].map(lambda x: 0 if x == \"male\" else 1)\n",
        "# feature_Sex[\"Female\"] = train_data[\"Sex\"].map(lambda x: 0 if x == \"female\" else 1)\n",
        "# type(feature_Sex)\n",
        "# feature_Sex.drop(columns=[\"Sex\"], axis=1, inplace=True)\n",
        "# feature_Sex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6LazEzBcA4E"
      },
      "source": [
        "### Feature Pclass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": true,
        "id": "NuodM_dCcEgg"
      },
      "outputs": [],
      "source": [
        "# train_data[\"Pclass\"].unique()\n",
        "\n",
        "def featureMK_Pclass(df: pd.DataFrame):\n",
        "  feature_Pclass = df[[\"Pclass\"]].copy()\n",
        "  feature_Pclass[\"Pclass1\"] = df[\"Pclass\"].map(lambda x: 1 if x == 1 else 0)\n",
        "  feature_Pclass[\"Pclass2\"] = df[\"Pclass\"].map(lambda x: 1 if x == 2 else 0)\n",
        "  feature_Pclass[\"Pclass3\"] = df[\"Pclass\"].map(lambda x: 1 if x == 3 else 0)\n",
        "  feature_Pclass.drop(columns=[\"Pclass\"], axis=1, inplace=True)\n",
        "  return feature_Pclass\n",
        "\n",
        "# feature_Pclass = train_data[[\"Pclass\"]].copy()\n",
        "# feature_Pclass[\"Pclass1\"] = train_data[\"Pclass\"].map(lambda x: 1 if x == 1 else 0)\n",
        "# feature_Pclass[\"Pclass2\"] = train_data[\"Pclass\"].map(lambda x: 2 if x == 2 else 0)\n",
        "# feature_Pclass[\"Pclass3\"] = train_data[\"Pclass\"].map(lambda x: 3 if x == 3 else 0)\n",
        "# type(feature_Pclass)\n",
        "# feature_Pclass.drop(columns=[\"Pclass\"], axis=1, inplace=True)\n",
        "# feature_Pclass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRKZWHLXenec"
      },
      "source": [
        "### Feature Fare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "pB27wyF2eq-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f157c0f-cf03-4085-ffad-15a44bb0b43c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Fare    float64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "import math\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler, Normalizer\n",
        "# train_data[\"Fare\"].map(lambda x: math.log(x+1))\n",
        "\n",
        "def featureMK_Fare(df: pd.DataFrame):\n",
        "  # feature_Fare = df[\"Fare\"].map(lambda x: math.log(x+1))\n",
        "  # return feature_Fare\n",
        "\n",
        "  x = df[\"Fare\"] #returns a numpy array\n",
        "  # min_max_scaler = MinMaxScaler()\n",
        "  # x_scaled = min_max_scaler.fit_transform([x])\n",
        "  # dfx = pd.DataFrame(x_scaled)\n",
        "  # dfx\n",
        "\n",
        "  norm = Normalizer()\n",
        "  x_norm = norm.fit_transform([x])\n",
        "  dfx = pd.DataFrame(x_norm).T\n",
        "  dfx.columns = [\"Fare\"]\n",
        "  return dfx\n",
        "\n",
        "# feature_Fare = train_data[\"Fare\"].map(lambda x: math.log(x+1))\n",
        "# feature_Fare\n",
        "\n",
        "\n",
        "tmp = featureMK_Fare(train_data)\n",
        "type(tmp)\n",
        "tmp.dtypes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA_NY2OygyLa"
      },
      "source": [
        "### Feature Embarked"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": true,
        "id": "bZFSlNGDg1ZV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def featureMK_Embarked(df: pd.DataFrame):\n",
        "  feature_Embarked = df[[\"Embarked\"]].copy()\n",
        "  feature_Embarked[\"Embarked_S\"] = df[\"Embarked\"].map(lambda x: 1 if x == \"S\" else 0)\n",
        "  feature_Embarked[\"Embarked_C\"] = df[\"Embarked\"].map(lambda x: 1 if x == \"C\" else 0)\n",
        "  feature_Embarked[\"Embarked_Q\"] = df[\"Embarked\"].map(lambda x: 1 if x == \"Q\" else 0)\n",
        "  feature_Embarked.drop(columns=[\"Embarked\"], axis=1, inplace=True)\n",
        "  return feature_Embarked\n",
        "\n",
        "# feature_Embarked = train_data[[\"Embarked\"]].copy()\n",
        "# feature_Embarked[\"Embarked_S\"] = train_data[\"Embarked\"].map(lambda x: 1 if x == \"S\" else 0)\n",
        "# feature_Embarked[\"Embarked_C\"] = train_data[\"Embarked\"].map(lambda x: 1 if x == \"C\" else 0)\n",
        "# feature_Embarked[\"Embarked_Q\"] = train_data[\"Embarked\"].map(lambda x: 1 if x == \"Q\" else 0)\n",
        "# type(feature_Embarked)\n",
        "# feature_Embarked.drop(columns=[\"Embarked\"], axis=1, inplace=True)\n",
        "# feature_Embarked"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXxNTNjskQC3"
      },
      "source": [
        "### Feature Cabin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "q_OBgaJIkR3n"
      },
      "outputs": [],
      "source": [
        "# train_data.duplicated(subset=[\"Cabin\"]).to_numpy()\n",
        "# train_data[\"Cabin\"].loc[train_data.duplicated(subset=[\"Cabin\"])].to_frame()\n",
        "# train_data[\"Cabin\"].map(lambda x: str(x)[0]).unique()\n",
        "\n",
        "def featureMK_Cabin(df: pd.DataFrame):\n",
        "  feature_Cabin = df[[\"Cabin\"]].copy()\n",
        "  feature_Cabin[\"CabinA\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'A' else 0)\n",
        "  feature_Cabin[\"CabinB\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'B' else 0)\n",
        "  feature_Cabin[\"CabinC\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'C' else 0)\n",
        "  feature_Cabin[\"CabinD\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'D' else 0)\n",
        "  feature_Cabin[\"CabinE\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'E' else 0)\n",
        "  feature_Cabin[\"CabinF\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'F' else 0)\n",
        "  feature_Cabin[\"CabinG\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'G' else 0)\n",
        "  feature_Cabin[\"CabinT\"] = df[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'T' else 0)\n",
        "  feature_Cabin.drop(columns=[\"Cabin\"], axis=1, inplace=True)\n",
        "\n",
        "# feature_Cabin = train_data[[\"Cabin\"]].copy()\n",
        "# feature_Cabin[\"CabinA\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'A' else 0)\n",
        "# feature_Cabin[\"CabinB\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'B' else 0)\n",
        "# feature_Cabin[\"CabinC\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'C' else 0)\n",
        "# feature_Cabin[\"CabinD\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'D' else 0)\n",
        "# feature_Cabin[\"CabinE\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'E' else 0)\n",
        "# feature_Cabin[\"CabinF\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'F' else 0)\n",
        "# feature_Cabin[\"CabinG\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'G' else 0)\n",
        "# feature_Cabin[\"CabinT\"] = train_data[\"Cabin\"].map(lambda x: 1 if str(x)[0] == 'T' else 0)\n",
        "# feature_Cabin.drop(columns=[\"Cabin\"], axis=1, inplace=True)\n",
        "# type(feature_Cabin)\n",
        "# feature_Cabin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQI-W05JVB-7"
      },
      "source": [
        "## Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "9HAqnkrXYLVz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d9d6efc-aa7b-4a6d-94c1-602dd36dd3d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PassengerId      int64\n",
              "Age            float64\n",
              "Male             int64\n",
              "Female           int64\n",
              "Pclass1          int64\n",
              "Pclass2          int64\n",
              "Pclass3          int64\n",
              "Fare           float64\n",
              "SibSp            int64\n",
              "Parch            int64\n",
              "Embarked_S       int64\n",
              "Embarked_C       int64\n",
              "Embarked_Q       int64\n",
              "dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ],
      "source": [
        "## TODO : Convert feature generation for functions returing series or DF\n",
        "\n",
        "# train_data_in = pd.concat([feature_PassengerId, feature_Age, feature_Sex, feature_Pclass, feature_Fare, train_data['SibSp'], train_data['Parch'], feature_Embarked, feature_Cabin], axis=1)\n",
        "train_data_in = pd.concat([feature_PassengerId, featureMK_Age(train_data), featureMK_Sex(train_data), featureMK_Pclass(train_data), featureMK_Fare(train_data), train_data['SibSp'], train_data['Parch'], featureMK_Embarked(train_data), featureMK_Cabin(train_data)], axis=1)\n",
        "# train_data_in = pd.concat([train_data_in, train_data['SibSp'], train_data['Parch']], axis=1)\n",
        "\n",
        "# test_data_in = pd.concat([test_data['PassengerId'], featureMK_Age(test_data), featureMK_Sex(test_data), featureMK_Pclass(test_data), featureMK_Fare(test_data), test_data['SibSp'], test_data['Parch'], featureMK_Embarked(test_data), featureMK_Cabin(test_data)], axis=1)\n",
        "\n",
        "\n",
        "# test_data_in = test_data[features_num]\n",
        "# test_data_in = test_data[[\"PassengerId\", \"Age\"]]\n",
        "train_data_in.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eGcmV4GzPyLH",
        "outputId": "e7625415-dfda-48c9-e2fc-1222754768f0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8212290502793296"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "\n",
        "\n",
        "# train_data_in = train_data[features_num]\n",
        "# test_data_in = test_data[features_num]\n",
        "\n",
        "train_X, test_X, train_y, test_y = train_test_split(train_data_in, train_label, test_size=0.2, random_state=0)\n",
        "\n",
        "# clf_rf = RandomForestClassifier(random_state=0)\n",
        "clf_rf = RandomForestClassifier(random_state=0)\n",
        "clf_rf.fit(train_X, train_y)\n",
        "clf_rf.score(test_X, test_y)\n",
        "\n",
        "# preds = clf_rf.predict(test_data_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "collapsed": true,
        "id": "KwXZH_pQlfP_",
        "outputId": "65870f22-f006-428f-c33a-145378f7e854"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Invalid parameter 'learning_rate' for estimator RandomForestClassifier(random_state=1). Valid parameters are: ['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'].",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-f6a18a68a059>\u001b[0m in \u001b[0;36m<cell line: 38>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0mGS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mmodelVar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mhgbr_search_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"neg_mean_absolute_error\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"r2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"neg_mean_absolute_error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'raise'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    819\u001b[0m                     )\n\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    822\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    823\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdelayed_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         )\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable_with_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_sequential_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1917\u001b[0m             \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_generator\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1920\u001b[0m         \u001b[0;31m# Let's create an ID that uniquely identifies the current call. If the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_batches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_dispatched_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_completed_tasks\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_progress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mcloned_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msafe\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcloned_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mset_params\u001b[0;34m(self, **params)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalid_params\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0mlocal_valid_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    206\u001b[0m                     \u001b[0;34mf\"Invalid parameter {key!r} for estimator {self}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                     \u001b[0;34mf\"Valid parameters are: {local_valid_params!r}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Invalid parameter 'learning_rate' for estimator RandomForestClassifier(random_state=1). Valid parameters are: ['bootstrap', 'ccp_alpha', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start']."
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "modelVar = HistGradientBoostingClassifier(random_state=1)\n",
        "\n",
        "\n",
        "search_space = {\n",
        "    \"n_estimators\": [100, 200, 300, 500],\n",
        "    \"max_depth\": [2, 5, 7],\n",
        "    # \"gamma\": [0.01, 0.1],\n",
        "    \"learning_rate\": [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "hgbr_search_space = {\n",
        "    # \"loss\": [\"squared_error\", \"absolute_error\"],\n",
        "    \"learning_rate\": [0.001, 0.01, 0.1, 1],\n",
        "    \"max_iter\": [100, 200, 300],\n",
        "    \"max_depth\": [2, 5, 7],\n",
        "}\n",
        "\n",
        "rf_search_space = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"criterion\": [\"gini\", \"entropy\", \"log_loss\"],\n",
        "    \"max_depth\": [2, 5, 7],\n",
        "    # \"gamma\": [0.01, 0.1],\n",
        "    # \"learning_rate\": [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "gb_search_space = {\n",
        "    \"n_estimators\": [100, 200, 300],\n",
        "    \"criterion\": [\"friedman_mse\", \"squared_error\"],\n",
        "    \"max_depth\": [2, 5, 7],\n",
        "    # \"gamma\": [0.01, 0.1],\n",
        "    \"learning_rate\": [0.001, 0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "GS = GridSearchCV(estimator= modelVar, param_grid= hgbr_search_space, cv=5, scoring=[\"neg_mean_absolute_error\",\"r2\"], refit=\"neg_mean_absolute_error\", verbose=4, error_score='raise')\n",
        "GS.fit(train_X, train_y)\n",
        "GS.best_params_\n",
        "\n",
        "modelVar.fit(train_X, train_y)\n",
        "modelVar.score(test_X, test_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajX23gohnEI6",
        "outputId": "cb7da608-7b61-4ecd-f006-55ddacfcb6eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'learning_rate': 0.1, 'max_depth': 2, 'max_iter': 300}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "modelVar.score(test_X, test_y)\n",
        "GS.best_params_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVlfOxjASsO_"
      },
      "source": [
        "# EDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iN71Tv1GSzOA"
      },
      "outputs": [],
      "source": [
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP+LSvwM7r2WXlRhu7Pwc2q",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}